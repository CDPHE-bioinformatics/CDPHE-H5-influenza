{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: <project_name>\n",
    "## sample subset: <primer_set>\n",
    "## analysis date: <analysis_date>\n",
    "## user: <name>\n",
    "\n",
    "### Description\n",
    "<High-level description of samples, primers, and references used>\n",
    "\n",
    "### General steps\n",
    "- prep fastq files\n",
    "- fastqc on raw fastq files\n",
    "- clean fastq files using seqyclean\n",
    "- fastqc on filtered fastq files\n",
    "- align using bwa\n",
    "- trim primers using ivar trim\n",
    "- generate consensus seqeunce with ivar consensus\n",
    "- look at bam files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all python modules\n",
    "# general\n",
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "import re\n",
    "\n",
    "#dates \n",
    "from datetime import date\n",
    "from datetime import time\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "# data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# biopython\n",
    "import Bio\n",
    "from Bio import SeqIO\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "\n",
    "from Bio import SeqIO\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from Bio.Seq import Seq\n",
    "from Bio.Align.Applications import MafftCommandline\n",
    "from Bio import AlignIO\n",
    "\n",
    "import subprocess\n",
    "\n",
    "DOCKER_BASE = 'docker run --rm -v ${PWD}:/data -u $(id -u):$(id -g)'\n",
    "FASTQC_DOCKER = f'{DOCKER_BASE} staphb/fastqc:0.12~/flu/h5/test_h5_0005_nextseq_template_test/avrl_h5n1_250bp/seqyclean.1'\n",
    "MULTIQC_DOCKER = f'{DOCKER_BASE} staphb/multiqc:1.8'\n",
    "SEQYCLEAN_DOCKER = f'{DOCKER_BASE} staphb/seqyclean:1.10.09'\n",
    "BWA_DOCKER = f'{DOCKER_BASE} staphb/bwa:0.7.17'\n",
    "SAMTOOLS_DOCKER = f'{DOCKER_BASE} staphb/samtools:1.10'\n",
    "IVAR_DOCKER = f'{DOCKER_BASE} staphb/ivar:1.4.2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Set Up\n",
    "\n",
    "Edit the below input variables as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# working directory absolute path \n",
    "wd = '/home/sam_baird/flu/h5/test_h5_0005_nextseq_template_test/avrl_h5n1_250bp/'  # e.g. /home/sam_baird/flu/h5/test_h5_0005_nextseq/avrl_h5n1_250bp/\n",
    "\n",
    "project_name = 'test_h5_0005_nextseq' # e.g. test_h5_0005_nextseq\n",
    "\n",
    "### INPUT FILES ###\n",
    "# just provide the filenames, no paths\n",
    "\n",
    "# if multiple primer sets in single project, use subworkbook split by primer_set\n",
    "workbook = 'test_h5_0005_nextseq_workbook_avrl_h5n1_250bp.tsv' # e.g. test_h5_0005_nextseq_workbook_avrl_h5n1_250bp.tsv\n",
    "\n",
    "seqyclean_contaminants_fasta = 'Adapters_plus_PhiX_174.fasta'\n",
    "\n",
    "primer_bed = 'AVRL_H5N1_250bpAmpWGS_v1.bed'\n",
    "\n",
    "# keys: short name (used for naming subdirectories and files)\n",
    "# values: filename\n",
    "# references can be either single gene segments or multifastas\n",
    "reference_sequences = {\n",
    "    'h5n1_bovine': 'A_Bovine_texas_029328-01_UtoT.fasta',  # e.g. 'h5n1_bovine': 'A_Bovine_texas_029328-01_UtoT.fasta'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up directories and paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os.chdir(wd)\n",
    "refs_dir = 'references'\n",
    "if not os.path.exists(refs_dir):\n",
    "    os.mkdir(refs_dir)\n",
    "reference_sequence_paths = {k: os.path.join(refs_dir, v) for k, v in reference_sequences.items()}\n",
    "\n",
    "fastq_dir = 'fastq_files'\n",
    "if not os.path.exists(fastq_dir):\n",
    "    os.mkdir(fastq_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Manual steps needed**: In the working directory, add the:\n",
    "- workbook\n",
    "- FASTQ files to the `fastq_files` directory\n",
    "- reference genomes to the `references` directory\n",
    "- seqyclean contaminants FASTA to the `references` directory\n",
    "- primer BED file to the `references` directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read workbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path_wb = os.path.join(wd, workbook)\n",
    "wb = pd.read_csv(path_wb, sep = '\\t')\n",
    "\n",
    "sample_names = wb.sample_name.tolist()\n",
    "wb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastQC on raw FASTQ files\n",
    "Assess the quality of the raw FASTQ files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastqc_raw_dir = os.path.join(wd, 'fastqc_raw')\n",
    "if not os.path.exists(fastqc_raw_dir):\n",
    "    os.mkdir(fastqc_raw_dir)\n",
    "\n",
    "for sample in sample_names:\n",
    "    print(sample)\n",
    "    \n",
    "    fastq_1 = f'{sample}_R1.fastq.gz'\n",
    "    fastq_2 = f'{sample}_R2.fastq.gz'\n",
    "\n",
    "    outdir = os.path.join(fastqc_raw_dir, sample)\n",
    "    if not os.path.exists(outdir):\n",
    "        os.mkdir(outdir)\n",
    "        \n",
    "    !{FASTQC_DOCKER} fastqc --outdir fastqc_raw/{sample} fastq_files/{fastq_1}\n",
    "    !{FASTQC_DOCKER} fastqc --outdir fastqc_raw/{sample} fastq_files/{fastq_2}\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiQC on raw FASTQ files\n",
    "Combine FastQC results into a single report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiqc_raw_dir = 'multiqc_raw'\n",
    "if not os.path.exists(multiqc_raw_dir):\n",
    "    os.mkdir(multiqc_raw_dir)\n",
    "\n",
    "!{MULTIQC_DOCKER} multiqc fastqc_raw --outdir {multiqc_raw_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSV report on raw FASTQ files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_reads(sample_names, fastqc_dir):\n",
    "    for sample in sample_names:\n",
    "        print(sample)\n",
    "        # get fastqc path for zip files\n",
    "        zip1 = glob.glob(os.path.join(fastqc_dir, sample, f'{sample}*1_fastqc.zip'))[0]\n",
    "        zip2 = glob.glob(os.path.join(fastqc_dir, sample, f'{sample}*2_fastqc.zip'))[0]\n",
    "        \n",
    "        # set unzip outpath\n",
    "        unzip_outpath = os.path.join(fastqc_dir, sample)\n",
    "        \n",
    "        # unzip\n",
    "        !unzip -q -o {zip1} -d {unzip_outpath}\n",
    "        !unzip -q -o {zip2} -d {unzip_outpath}\n",
    "        \n",
    "        # read in file and create summary out file\n",
    "        # set up dataframe\n",
    "        df = pd.DataFrame()\n",
    "        df['sample_name'] = [sample]\n",
    "        \n",
    "        # R1\n",
    "        path = glob.glob(os.path.join(unzip_outpath, f'{sample}*_fastqc', 'fastqc_data.txt'))[0]\n",
    "        \n",
    "        command = f'cat {path} | grep \"Total Sequences\" | cut -f 2'\n",
    "        total_seqs = subprocess.check_output(command, shell=True, text=True).rstrip('\\n')\n",
    "        df['r1_total_reads'] = [total_seqs]\n",
    "\n",
    "        command = f'cat {path} | grep \"Sequences flagged as poor quality\" | cut -f 2'\n",
    "        flagged_seqs = subprocess.check_output(command, shell=True, text=True).rstrip('\\n')\n",
    "        df['r1_flagged_reads_as_poor_quality'] = [flagged_seqs]\n",
    "\n",
    "        command = f'cat {path} | grep \"Sequence length\" | cut -f 2'\n",
    "        seq_len = subprocess.check_output(command, shell=True, text=True).rstrip('\\n')\n",
    "        df['r1_read_len'] = [seq_len]\n",
    "        \n",
    "        # R2\n",
    "        path = glob.glob(os.path.join(unzip_outpath, f'{sample}*2_fastqc', 'fastqc_data.txt'))[0]\n",
    "        \n",
    "        command = f'cat {path} | grep \"Total Sequences\" | cut -f 2'\n",
    "        total_seqs = subprocess.check_output(command, shell = True, text = True).rstrip('\\n')\n",
    "        df['r2_total_reads'] = [total_seqs]\n",
    "\n",
    "        command = f'cat {path} | grep \"Sequences flagged as poor quality\" | cut -f 2'\n",
    "        flagged_seqs = subprocess.check_output(command, shell = True, text = True).rstrip('\\n')\n",
    "        df['r2_flagged_reads_as_poor_quality'] = [flagged_seqs]\n",
    "\n",
    "        command = f'cat {path} | grep \"Sequence length\" | cut -f 2'\n",
    "        seq_len = subprocess.check_output(command, shell = True, text = True).rstrip('\\n')\n",
    "        df['r2_read_len'] = [seq_len]\n",
    "        \n",
    "        outfile = os.path.join(unzip_outpath, f'{sample}_summary_metrics.tsv')\n",
    "        df.to_csv(outfile, sep = '\\t', index = False)\n",
    "\n",
    "summarize_reads(sample_names, fastqc_raw_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seqyclean on raw FASTQ files\n",
    "Filter by quality and remove known contaminants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqyclean_dir = 'seqyclean'\n",
    "if not os.path.exists(seqyclean_dir):\n",
    "    os.mkdir(seqyclean_dir)\n",
    "\n",
    "seqyclean_commands = []\n",
    "for sample in sample_names:    \n",
    "    fastq_1 = f'fastq_files/{sample}_R1.fastq.gz'\n",
    "    fastq_2 = f'fastq_files/{sample}_R2.fastq.gz'         \n",
    "    out_name = f'seqyclean/{sample}_clean'\n",
    "\n",
    "    command = f'{SEQYCLEAN_DOCKER} seqyclean -minlen 25 -qual 30 30 -gz -1 {fastq_1} -2 {fastq_2} -c references/Adapters_plus_PhiX_174.fasta -o {out_name}'\n",
    "    seqyclean_commands.append(command)\n",
    "\n",
    "seqyclean_commands = '\\n' + '\\n'.join(seqyclean_commands) + '\\n'\n",
    "! echo {seqyclean_commands} | parallel "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastQC on cleaned FASTQ files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastqc_clean_dir = os.path.join(wd, 'fastqc_clean')\n",
    "if not os.path.exists(fastqc_clean_dir):\n",
    "    os.mkdir(fastqc_clean_dir)\n",
    "\n",
    "for sample in sample_names:\n",
    "    print(sample)\n",
    "    \n",
    "    fastq_1 = f'{sample}_clean_PE1.fastq.gz'\n",
    "    fastq_2 = f'{sample}_clean_PE2.fastq.gz'\n",
    "\n",
    "    outdir = os.path.join(fastqc_clean_dir, sample)\n",
    "    if not os.path.exists(outdir):\n",
    "        os.mkdir(outdir)\n",
    "                \n",
    "    !{FASTQC_DOCKER} fastqc --outdir fastqc_clean/{sample} seqyclean/{fastq_1}\n",
    "    !{FASTQC_DOCKER} fastqc --outdir fastqc_clean/{sample} seqyclean/{fastq_2}\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiQC on cleaned FASTQ files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiqc_clean_dir = 'multiqc_clean'\n",
    "if not os.path.exists(multiqc_clean_dir):\n",
    "    os.mkdir(multiqc_clean_dir)\n",
    "\n",
    "!{MULTIQC_DOCKER} multiqc fastqc_clean --outdir {multiqc_clean_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSV report on raw FASTQ files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_reads_summary = summarize_reads(sample_names, fastqc_clean_dir)\n",
    "clean_reads_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get summary metrics on reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine fastqc sumary from raw and filtered data\n",
    "df_raw_list = []\n",
    "df_filtered_list = []\n",
    "    \n",
    "    \n",
    "for sample in sample_names:    \n",
    "    # first read in raw summary file\n",
    "    path = os.path.join(wd, 'fastqc_raw', sample, f'{sample}_summary_metrics.tsv')\n",
    "    df = pd.read_csv(path, sep = '\\t')\n",
    "    \n",
    "    df_raw_list.append(df)\n",
    "    \n",
    "    # now read in filtered summary file\n",
    "    path = os.path.join(wd, 'fastqc_clean', sample, f'{sample}_summary_metrics.tsv')\n",
    "    df = pd.read_csv(path, sep = '\\t')\n",
    "    \n",
    "    df_filtered_list.append(df)\n",
    "    \n",
    "    \n",
    "# concatenate dataframes and join\n",
    "raw_df = pd.concat(df_raw_list).reset_index(drop = True)\n",
    "raw_df = raw_df.set_index('sample_name')\n",
    "\n",
    "filtered_df = pd.concat(df_filtered_list).reset_index(drop = True)\n",
    "filtered_df = filtered_df.set_index('sample_name')\n",
    "\n",
    "\n",
    "combined_df = raw_df.join(filtered_df, how = 'left', lsuffix = '_raw', rsuffix = '_filtered')\n",
    "combined_df = combined_df.reset_index()\n",
    "\n",
    "# calculate total reads diff\n",
    "combined_df['total_reads_diff'] = combined_df['r1_total_reads_raw'] - combined_df['r1_total_reads_filtered']\n",
    "combined_df['raw_total_reads_paired'] = combined_df['r1_total_reads_raw']\n",
    "combined_df['filtered_total_reads_paired'] = combined_df['r1_total_reads_filtered']\n",
    "\n",
    "# add primer set\n",
    "for row in range(combined_df.shape[0]):\n",
    "    sample = combined_df.sample_name[row]\n",
    "\n",
    "# print(j.columns)\n",
    "\n",
    "col_order = ['sample_name', 'raw_total_reads_paired', 'filtered_total_reads_paired',\n",
    "             'total_reads_diff', 'r1_total_reads_raw', 'r1_flagged_reads_as_poor_quality_raw',\n",
    "             'r1_read_len_raw', 'r2_total_reads_raw', 'r2_flagged_reads_as_poor_quality_raw',\n",
    "             'r2_read_len_raw', 'r1_total_reads_filtered', 'r1_flagged_reads_as_poor_quality_filtered',\n",
    "             'r1_read_len_filtered', 'r2_total_reads_filtered',\n",
    "             'r2_flagged_reads_as_poor_quality_filtered', 'r2_read_len_filtered']\n",
    "\n",
    "combined_df = combined_df[col_order]\n",
    "combined_df['project_name'] = project_name\n",
    "\n",
    "outfile = os.path.join(wd, f'{project_name}_reads_QC_summary.csv')\n",
    "combined_df.to_csv(outfile, index = False)\n",
    "combined_df\n",
    "\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Align to each reference using BWA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alignment_dir = 'bwa_alignment'\n",
    "if not os.path.exists(alignment_dir):\n",
    "    os.mkdir(alignment_dir)\n",
    "\n",
    "for sample in sample_names:\n",
    "    # align each sample to each reference\n",
    "    for ref in reference_sequence_paths:\n",
    "\n",
    "        fastq_1 = os.path.join(seqyclean_dir, f'{sample}_clean_PE1.fastq.gz')\n",
    "        fastq_2 = os.path.join(seqyclean_dir, f'{sample}_clean_PE2.fastq.gz')\n",
    "\n",
    "        ref_base_name = ref.split('/')[-1]\n",
    "\n",
    "        # each reference gets its own subdirectory\n",
    "        out_subdir = os.path.join(alignment_dir, ref_base_name)\n",
    "        if not os.path.exists(out_subdir):\n",
    "            os.mkdir(out_subdir)\n",
    "\n",
    "        outsam = os.path.join(out_subdir, f'{sample}.sam')\n",
    "        outbam = os.path.join(out_subdir, f'{sample}.aln.sorted.bam')\n",
    "\n",
    "        !{BWA_DOCKER} bwa index -p {ref_base_name} -a is {reference_sequence_paths[ref]}\n",
    "        !{BWA_DOCKER} bwa mem -t 6 {ref_base_name} {fastq_1} {fastq_2} -f {outsam}\n",
    "        !{SAMTOOLS_DOCKER} samtools view -bS {outsam} | samtools sort -o {outbam}\n",
    "        !rm {outsam}  # save storage space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trim primers from alignment using ivar trim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_alignment_dir = 'bwa_alignment'\n",
    "trimmed_alignment_dir = 'bwa_alignment_trimmed'\n",
    "\n",
    "for sample in sample_names:\n",
    "    for ref in reference_sequence_paths:\n",
    "        primer_bed_path = os.path.join(refs_dir, primer_bed)\n",
    "\n",
    "        out_subdir = os.path.join(trimmed_alignment_dir, ref)\n",
    "        os.makedirs(out_subdir, exist_ok=True)\n",
    "\n",
    "        in_subdir = os.path.join(raw_alignment_dir, ref)\n",
    "        aln_bam = os.path.join(in_subdir, f'{sample}.aln.sorted.bam')\n",
    "\n",
    "        trimmed_bam = os.path.join(out_subdir, f'{sample}_trimmed.bam')\n",
    "        trimmed_sorted_bam = os.path.join(out_subdir, f'{sample}_trimmed.sorted.bam')\n",
    "\n",
    "        !{IVAR_DOCKER} ivar trim -e -i {aln_bam} -b {primer_bed_path} -p {trimmed_bam}\n",
    "        !{SAMTOOLS_DOCKER} samtools sort {trimmed_bam} -o {trimmed_sorted_bam}\n",
    "        !{SAMTOOLS_DOCKER} samtools index {trimmed_sorted_bam}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO Determine consensus sequence using ivar consensus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consensus_dir = 'consensus_sequences'\n",
    "if not os.path.exists(consensus_dir):\n",
    "    os.mkdir(consensus_dir)\n",
    "\n",
    "for sample in sample_names:\n",
    "    for ref in reference_sequence_paths:\n",
    "        out_subdir_consensus = os.path.join(consensus_dir, ref)\n",
    "        if not os.path.exists(out_subdir_consensus):\n",
    "            os.mkdir(out_subdir_consensus)\n",
    "\n",
    "        trimmed_sorted_bam = os.path.join(trimmed_alignment_dir, ref, f'{sample}_trimmed.sorted.bam')\n",
    "        pileup_txt = os.path.join(trimmed_alignment_dir, ref, f'{sample}_pileup.txt')\n",
    "\n",
    "        generate pileup\n",
    "        !{SAMTOOLS_DOCKER} samtools faidx {reference_sequence_paths[ref]}\n",
    "        !{SAMTOOLS_DOCKER} samtools mpileup -A -aa -d 600000 -B -Q 20 -q 20 -f {reference_sequence_paths[ref]} {trimmed_sorted_bam} -o {pileup_txt}\n",
    "\n",
    "        # if reference is a multifasta, write separate consensus for each gene segment\n",
    "        records = SeqIO.parse(reference_sequence_paths[ref], format='fasta')\n",
    "        num_records = len(list(records))\n",
    "        if num_records > 1:\n",
    "            for segment in SeqIO.parse(reference_sequence_paths[ref], format='fasta'):\n",
    "                print('hi')\n",
    "                out_subdir_consensus = os.path.join(consensus_dir, ref, segment.id)\n",
    "                consensus_prefix = os.path.join(out_subdir_consensus, f'{sample}_{ref}_{segment.id}_consensus')\n",
    "                segment_id = segment.id\n",
    "                command = f\"cat {pileup_txt} | grep {segment_id} | ivar consensus -p {consensus_prefix} -q 20 -t 0.6 -m 10\"\n",
    "                print(command)\n",
    "                !{IVAR_DOCKER} sh -c \"{command}\"\n",
    "                \n",
    "        else:\n",
    "            out_subdir_consensus = os.path.join(consensus_dir, ref)\n",
    "            consensus_prefix = os.path.join(out_subdir_consensus, f'{sample}_{ref}_consensus')\n",
    "            !{IVAR_DOCKER} sh -c \"cat {pileup_txt} | ivar consensus -p {consensus_prefix} -q 20 -t 0.6 -m 10\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO Calculate alignment metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aln_metrics_dir = 'alignment_metrics'\n",
    "if not os.path.exists(aln_metrics_dir):\n",
    "    os.mkdir(aln_metrics_dir)\n",
    "    \n",
    "for sample in sample_names:\n",
    "    # grab bam file\n",
    "    aln_dir = os.path.join('bwa_alignment_trimmed', ref)\n",
    "    bam_file = os.path.join(aln_dir, f'{sample}_trimmed.sorted.bam')\n",
    "\n",
    "    # set outfile\n",
    "    out_subdir_aln_metrics = os.path.join(aln_metrics_dir, ref)\n",
    "    if not os.path.exists(out_subdir_aln_metrics):\n",
    "        os.mkdir(out_subdir_aln_metrics)\n",
    "\n",
    "    out_bam_coverage = os.path.join(out_subdir_aln_metrics, f'{sample}_coverage.txt')\n",
    "    out_bam_stats = os.path.join(out_subdir_aln_metrics, f'{sample}_stats.txt')\n",
    "\n",
    "    # run samtools\n",
    "    ! {SAMTOOLS_DOCKER} samtools coverage -o {out_bam_coverage} {bam_file}\n",
    "    ! {SAMTOOLS_DOCKER} sh -c \"samtools stats {bam_file} > {out_bam_stats}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO Calculate percent coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference length with and without primers\n",
    "# e.g.\n",
    "# reference_lengths = {\n",
    "reference_lengths = {}\n",
    "for ref in reference_sequence_paths:\n",
    "    records = SeqIO.parse(reference_sequence_paths[ref])\n",
    "    num_records = len(list(records))\n",
    "    bed_df = pd.read_csv(bed_file_path, sep='\\t', header=None)\n",
    "    if num_records > 1:\n",
    "        segment_lengths = {}\n",
    "        for segment in records:\n",
    "            total_length = len(segment.seq)\n",
    "            segment_bed_df = bed_df[bed_df[0] == segment.id]\n",
    "\n",
    "            # Span of primers is assumed to be the (highest end coordinate) - (lowest start coordinate) \n",
    "            primer_covered_length = segment_bed_df[2].max() - segment_bed_df[1].min()\n",
    "\n",
    "            segment_lengths[segment.id] = (total_length, primer_covered_length)\n",
    "        reference_lengths[ref] = segment_lengths\n",
    "    else:\n",
    "        total_length = len(records[0].seq)\n",
    "        primer_covered_length = bed_df[2].max() - bed_df[1].min()\n",
    "        reference_lengths[ref] = (total_length, primer_covered_length)\n",
    "\n",
    "reference_lengths"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
